{"cells":[{"cell_type":"markdown","metadata":{"id":"V7YOT2mnUiea"},"source":["**Wav2Vec2** is a pretrained model for Automatic Speech Recognition (ASR) and was released in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) by *Alexei Baevski, Michael Auli, and Alex Conneau*.  Soon after the superior performance of Wav2Vec2 was demonstrated on one of the most popular English datasets for ASR, called [LibriSpeech](https://huggingface.co/datasets/librispeech_asr), *Facebook AI* presented a multi-lingual version of Wav2Vec2, called [XLSR](https://arxiv.org/abs/2006.13979). XLSR stands for *cross-lingual speech representations* and refers to model's ability to learn speech representations that are useful across multiple languages.\n","\n","XLSR's successor, simply called **XLS-R** (refering to the [*''XLM-R*](https://ai.facebook.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/) *for Speech''*), was released in [November 2021](https://ai.facebook.com/blog/xls-r-self-supervised-speech-processing-for-128-languages) by *Arun Babu, Changhan Wang, Andros Tjandra, et al.* XLS-R used almost **half a million** hours of audio data in 128 languages for self-supervised pre-training and comes in sizes ranging from 300 milion up to **two billion** parameters. You can find the pretrained checkpoints on the 🤗 Hub:\n","\n","- [**Wav2Vec2-XLS-R-300M**](https://huggingface.co/facebook/wav2vec2-xls-r-300m)\n","- [**Wav2Vec2-XLS-R-1B**](https://huggingface.co/facebook/wav2vec2-xls-r-1b)\n","- [**Wav2Vec2-XLS-R-2B**](https://huggingface.co/facebook/wav2vec2-xls-r-2b)\n","\n","Similar to [BERT's masked language modeling objective](http://jalammar.github.io/illustrated-bert/), XLS-R learns contextualized speech representations by randomly masking feature vectors before passing them to a transformer network during self-supervised pre-training (*i.e.* diagram on the left below). \n","\n","For fine-tuning, a single linear layer is added on top of the pre-trained network to train the model on labeled data of audio downstream tasks such as speech recognition, speech translation and audio classification (*i.e.* diagram on the right below).\n","\n","![wav2vec2_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/xls_r.png)\n","\n","XLS-R shows impressive improvements over previous state-of-the-art results on both speech recognition, speech translation and speaker/language identification, *cf.* with Table 3-6, Table 7-10, and Table 11-12 respectively of the official [paper](https://ai.facebook.com/blog/xls-r-self-supervised-speech-processing-for-128-languages)."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T00:58:22.719485Z","iopub.status.busy":"2022-07-02T00:58:22.7191Z","iopub.status.idle":"2022-07-02T00:58:22.788573Z","shell.execute_reply":"2022-07-02T00:58:22.787444Z","shell.execute_reply.started":"2022-07-02T00:58:22.719449Z"},"id":"YELVqGxMxnbG","outputId":"1ab7eb67-409d-4371-b99e-7eb1171cb5fb","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Jul  7 17:42:39 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 511.79       Driver Version: 511.79       CUDA Version: 11.6     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n","| 54%   44C    P8    26W / 200W |   1137MiB /  8192MiB |     26%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A      1380    C+G                                   N/A      |\n","|    0   N/A  N/A      3736    C+G   ...264.49\\msedgewebview2.exe    N/A      |\n","|    0   N/A  N/A      4056    C+G   ...me\\Application\\chrome.exe    N/A      |\n","|    0   N/A  N/A      5388    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n","|    0   N/A  N/A      6548    C+G   C:\\Windows\\explorer.exe         N/A      |\n","|    0   N/A  N/A      8440    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n","|    0   N/A  N/A      8604    C+G   ...artMenuExperienceHost.exe    N/A      |\n","|    0   N/A  N/A      9364    C+G   ...ge\\Application\\msedge.exe    N/A      |\n","|    0   N/A  N/A      9708    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n","|    0   N/A  N/A     10500    C+G   ...n64\\EpicGamesLauncher.exe    N/A      |\n","|    0   N/A  N/A     11020    C+G                                   N/A      |\n","|    0   N/A  N/A     11092    C+G   ...ser\\Application\\brave.exe    N/A      |\n","|    0   N/A  N/A     11400    C+G   ...\\app-1.0.9005\\Discord.exe    N/A      |\n","|    0   N/A  N/A     11584    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n","|    0   N/A  N/A     11708    C+G                                   N/A      |\n","|    0   N/A  N/A     12340    C+G   ...s\\Win64\\EpicWebHelper.exe    N/A      |\n","|    0   N/A  N/A     12644    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n","|    0   N/A  N/A     14288    C+G   ...264.49\\msedgewebview2.exe    N/A      |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","    print('Not connected to a GPU')\n","else:\n","    print(gpu_info)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T00:58:22.790759Z","iopub.status.busy":"2022-07-02T00:58:22.790106Z","iopub.status.idle":"2022-07-02T00:58:59.461879Z","shell.execute_reply":"2022-07-02T00:58:59.460676Z","shell.execute_reply.started":"2022-07-02T00:58:22.79072Z"},"id":"c8eh87Hoee5d","trusted":true},"outputs":[],"source":["%%capture\n","!pip install datasets\n","!pip install transformers\n","!pip install torchaudio\n","!pip install jiwer"]},{"cell_type":"markdown","metadata":{"id":"0mW-C1Nt-j7k"},"source":["## Prepare Data, Tokenizer, Feature Extractor"]},{"cell_type":"markdown","metadata":{"id":"BeBosnY9BH3e"},"source":["ASR models transcribe speech to text, which means that we both need a feature extractor that processes the speech signal to the model's input format, *e.g.* a feature vector, and a tokenizer that processes the model's output format to text. \n","\n","In 🤗 Transformers, the XLS-R model is thus accompanied by both a tokenizer, called [Wav2Vec2CTCTokenizer](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#wav2vec2ctctokenizer), and a feature extractor, called [Wav2Vec2FeatureExtractor](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#wav2vec2featureextractor).\n","\n","Let's start by creating the tokenizer to decode the predicted output classes to the output transcription."]},{"cell_type":"markdown","metadata":{"id":"sEXEWEJGQPqD"},"source":["### Create `Wav2Vec2CTCTokenizer`"]},{"cell_type":"markdown","metadata":{"id":"bee4g9rpLxll"},"source":["Common Voice has many different splits including `invalidated`, which refers to data that was not rated as \"clean enough\" to be considered useful. In this notebook, we will only make use of the splits `\"train\"`, `\"validation\"` and `\"test\"`. "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T00:58:59.46603Z","iopub.status.busy":"2022-07-02T00:58:59.46533Z","iopub.status.idle":"2022-07-02T01:12:38.943652Z","shell.execute_reply":"2022-07-02T01:12:38.942543Z","shell.execute_reply.started":"2022-07-02T00:58:59.465993Z"},"id":"2MMXcWFFgCXU","outputId":"00961862-9e79-4e0e-c887-db62adafa553","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\tonmo\\.conda\\envs\\open-mmlab\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Reusing dataset cvbn (C:\\Users\\tonmo\\.cache\\huggingface\\datasets\\bengaliAI___cvbn\\bn\\9.0.0\\87d0f829e4c25e6138fccec70a352eb556b9093ccb3d68714983c4d1fe983424)\n","100%|██████████| 3/3 [00:00<00:00, 21.58it/s]\n"]}],"source":["from datasets import load_dataset, load_metric, Audio\n","dataset=load_dataset(\"bengaliAI/cvbn\", \"bn\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ri5y5N_HMANq"},"source":["Many ASR datasets only provide the target text, `'sentence'` for each audio array `'audio'` and file `'path'`. Common Voice actually provides much more information about each audio file, such as the `'accent'`, etc. Keeping the notebook as general as possible, we only consider the transcribed text for fine-tuning.\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:12:38.946517Z","iopub.status.busy":"2022-07-02T01:12:38.945525Z","iopub.status.idle":"2022-07-02T01:12:38.951562Z","shell.execute_reply":"2022-07-02T01:12:38.950246Z","shell.execute_reply.started":"2022-07-02T01:12:38.946476Z"},"trusted":true},"outputs":[],"source":["common_voice_train=dataset[\"train\"]\n","common_voice_test=dataset[\"validation\"]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:12:38.953916Z","iopub.status.busy":"2022-07-02T01:12:38.953256Z","iopub.status.idle":"2022-07-02T01:12:39.039191Z","shell.execute_reply":"2022-07-02T01:12:39.038135Z","shell.execute_reply.started":"2022-07-02T01:12:38.953878Z"},"id":"kbyq6lDgQc2a","trusted":true},"outputs":[],"source":["common_voice_train = common_voice_train.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n","common_voice_test = common_voice_test.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])"]},{"cell_type":"markdown","metadata":{"id":"Go9Hq4e4NDT9"},"source":["Let's write a short function to display some random samples of the dataset and run it a couple of times to get a feeling for the transcriptions."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:14:42.634157Z","iopub.status.busy":"2022-07-02T01:14:42.633792Z","iopub.status.idle":"2022-07-02T01:14:42.642558Z","shell.execute_reply":"2022-07-02T01:14:42.641331Z","shell.execute_reply.started":"2022-07-02T01:14:42.634127Z"},"id":"72737oog2F6U","trusted":true},"outputs":[],"source":["from datasets import ClassLabel\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","def show_random_elements(dataset, num_examples=10):\n","    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","        picks.append(pick)\n","    \n","    df = pd.DataFrame(dataset[picks])\n","    display(HTML(df.to_html()))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:14:45.914368Z","iopub.status.busy":"2022-07-02T01:14:45.914015Z","iopub.status.idle":"2022-07-02T01:14:45.984098Z","shell.execute_reply":"2022-07-02T01:14:45.983107Z","shell.execute_reply.started":"2022-07-02T01:14:45.914338Z"},"id":"K_JUmf3G3b9S","outputId":"e3a0d9c8-4b68-4255-fd29-8dba65632a24","trusted":true},"outputs":[{"data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>তালেবান সরকারকে আনুষ্ঠানিক স্বীকৃতি দিয়েছিলো মাত্র তিনটি দেশ: পাকিস্তান, সৌদি আরব এবং সংযুক্ত আরব আমিরাত।</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>তার মধ্যে যুক্তরাষ্ট্রের কংগ্রেস, যুক্তরাজ্যের রয়াল সোসাইটি, যুক্তরাজ্যের পার্লামেন্ট, যুক্তরাষ্ট্রের জাতীয় বিজ্ঞান একাডেমি বিশেষ ভাবে উল্লেখযোগ্য।</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>তার জন্ম হয়েছিল নদিয়া জেলার কৃষ্ণনগরে।</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>পরিবার থেকেই তাদেরকে বিজ্ঞান চর্চায় অংশগ্রহণ কিংবা উদ্বুদ্ধ করা হয়নি।</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\"তার নামের অর্থ \"\"মহৎ নারীদের প্রধান\"\"।\"</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>এই সিনেমাটি মধ্যবিত্ত পরিবারের বিভিন্ন সাধারণ ও জটিল সমস্যা নিয়ে তৈরি।</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>ফলশ্রুতিতে, জাতীয় দল নির্বাচকমণ্ডলীর দৃষ্টি আকর্ষণে সক্ষম হন।</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>কাহিনীটি আনোয়ার হুসেইন রচনা করেন।</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>যা সংগ্রহ করার জন্য হাসানকে তার নিজস্ব ট্যাক্স এজেন্ট প্রেরণ করতে বলেছিলেন।</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>শঞ্জয় হায়াৎ আজাদ কাশ্মীরের মিরপুরের বাসিন্দা, কিন্তু তার জন্ম হয়েছে পাকিস্তানের ইসলামাবাদে।</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["show_random_elements(common_voice_train.remove_columns([\"path\", \"audio\"]), num_examples=10)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:14:49.231665Z","iopub.status.busy":"2022-07-02T01:14:49.231284Z","iopub.status.idle":"2022-07-02T01:14:49.237539Z","shell.execute_reply":"2022-07-02T01:14:49.236625Z","shell.execute_reply.started":"2022-07-02T01:14:49.231634Z"},"id":"svKzVJ_hQGK6","trusted":true},"outputs":[],"source":["import re\n","chars_to_remove_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\']'\n","\n","def remove_special_characters(batch):\n","    batch[\"sentence\"] = re.sub(chars_to_remove_regex, '', batch[\"sentence\"]).lower()\n","    return batch"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:14:53.83902Z","iopub.status.busy":"2022-07-02T01:14:53.838064Z","iopub.status.idle":"2022-07-02T01:16:28.956807Z","shell.execute_reply":"2022-07-02T01:16:28.955761Z","shell.execute_reply.started":"2022-07-02T01:14:53.83898Z"},"id":"XIHocAuTQbBR","outputId":"32b09453-6f60-42c4-e417-4e87edad58dd","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Parameter 'function'=<function remove_special_characters at 0x00000199D1986310> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","100%|██████████| 206951/206951 [00:13<00:00, 15358.29ex/s]\n","100%|██████████| 7748/7748 [00:00<00:00, 15073.91ex/s]\n"]}],"source":["common_voice_train = common_voice_train.map(remove_special_characters)\n","common_voice_test = common_voice_test.map(remove_special_characters)"]},{"cell_type":"markdown","metadata":{"id":"3ORHDb2Th2TW"},"source":["In CTC, it is common to classify speech chunks into letters, so we will do the same here. \n","Let's extract all distinct letters of the training and test data and build our vocabulary from this set of letters.\n","\n","We write a mapping function that concatenates all transcriptions into one long transcription and then transforms the string into a set of chars. \n","It is important to pass the argument `batched=True` to the `map(...)` function so that the mapping function has access to all transcriptions at once."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:16:32.128182Z","iopub.status.busy":"2022-07-02T01:16:32.127575Z","iopub.status.idle":"2022-07-02T01:16:32.133695Z","shell.execute_reply":"2022-07-02T01:16:32.132836Z","shell.execute_reply.started":"2022-07-02T01:16:32.128144Z"},"id":"LwCshNbbeRZR","trusted":true},"outputs":[],"source":["def extract_all_chars(batch):\n","    all_text = \" \".join(batch[\"sentence\"])\n","    vocab = list(set(all_text))\n","    return {\"vocab\": [vocab], \"all_text\": [all_text]}"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:16:34.750264Z","iopub.status.busy":"2022-07-02T01:16:34.749909Z","iopub.status.idle":"2022-07-02T01:17:22.0006Z","shell.execute_reply":"2022-07-02T01:17:21.999445Z","shell.execute_reply.started":"2022-07-02T01:16:34.750233Z"},"id":"_m6uUjjcfbjH","outputId":"67fa96dd-2df9-4393-f551-4d599c42efff","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:01<00:00,  1.94s/ba]\n","100%|██████████| 1/1 [00:00<00:00, 12.34ba/s]\n"]}],"source":["vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=False, remove_columns=common_voice_train.column_names)\n","vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=False, remove_columns=common_voice_test.column_names)"]},{"cell_type":"markdown","metadata":{"id":"7oVgE8RZSJNP"},"source":["Now, we create the union of all distinct letters in the training dataset and test dataset and convert the resulting list into an enumerated dictionary."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:17:22.005057Z","iopub.status.busy":"2022-07-02T01:17:22.004789Z","iopub.status.idle":"2022-07-02T01:17:22.012708Z","shell.execute_reply":"2022-07-02T01:17:22.01003Z","shell.execute_reply.started":"2022-07-02T01:17:22.005033Z"},"id":"aQfneNsmlJI0","trusted":true},"outputs":[],"source":["vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:17:22.01572Z","iopub.status.busy":"2022-07-02T01:17:22.015314Z","iopub.status.idle":"2022-07-02T01:17:22.113041Z","shell.execute_reply":"2022-07-02T01:17:22.111844Z","shell.execute_reply.started":"2022-07-02T01:17:22.015687Z"},"id":"_0kRndSvqaKk","outputId":"63440dfc-3f58-42c1-db78-3369f96f103f","trusted":true},"outputs":[{"data":{"text/plain":["{' ': 0,\n"," '/': 1,\n"," '=': 2,\n"," 'a': 3,\n"," 'b': 4,\n"," 'v': 5,\n"," '©': 6,\n"," '।': 7,\n"," '॥': 8,\n"," 'ঁ': 9,\n"," 'ং': 10,\n"," 'ঃ': 11,\n"," 'অ': 12,\n"," 'আ': 13,\n"," 'ই': 14,\n"," 'ঈ': 15,\n"," 'উ': 16,\n"," 'ঊ': 17,\n"," 'ঋ': 18,\n"," 'এ': 19,\n"," 'ঐ': 20,\n"," 'ও': 21,\n"," 'ঔ': 22,\n"," 'ক': 23,\n"," 'খ': 24,\n"," 'গ': 25,\n"," 'ঘ': 26,\n"," 'ঙ': 27,\n"," 'চ': 28,\n"," 'ছ': 29,\n"," 'জ': 30,\n"," 'ঝ': 31,\n"," 'ঞ': 32,\n"," 'ট': 33,\n"," 'ঠ': 34,\n"," 'ড': 35,\n"," 'ঢ': 36,\n"," 'ণ': 37,\n"," 'ত': 38,\n"," 'থ': 39,\n"," 'দ': 40,\n"," 'ধ': 41,\n"," 'ন': 42,\n"," 'প': 43,\n"," 'ফ': 44,\n"," 'ব': 45,\n"," 'ভ': 46,\n"," 'ম': 47,\n"," 'য': 48,\n"," 'র': 49,\n"," 'ল': 50,\n"," 'শ': 51,\n"," 'ষ': 52,\n"," 'স': 53,\n"," 'হ': 54,\n"," '়': 55,\n"," 'া': 56,\n"," 'ি': 57,\n"," 'ী': 58,\n"," 'ু': 59,\n"," 'ূ': 60,\n"," 'ৃ': 61,\n"," 'ে': 62,\n"," 'ৈ': 63,\n"," 'ো': 64,\n"," 'ৌ': 65,\n"," '্': 66,\n"," 'ৎ': 67,\n"," 'ড়': 68,\n"," 'ঢ়': 69,\n"," 'য়': 70,\n"," 'ৰ': 71,\n"," '৵': 72,\n"," '৷': 73,\n"," '–': 74,\n"," '—': 75,\n"," '’': 76,\n"," '‚': 77,\n"," '…': 78}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["vocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\n","vocab_dict"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:17:22.116757Z","iopub.status.busy":"2022-07-02T01:17:22.115644Z","iopub.status.idle":"2022-07-02T01:17:22.121731Z","shell.execute_reply":"2022-07-02T01:17:22.120484Z","shell.execute_reply.started":"2022-07-02T01:17:22.116718Z"},"id":"npbIbBoLgaFX","trusted":true},"outputs":[],"source":["vocab_dict[\"|\"] = vocab_dict[\" \"]\n","del vocab_dict[\" \"]"]},{"cell_type":"markdown","metadata":{"id":"_9yCWg4Sd0cb"},"source":["Finally, we also add a padding token that corresponds to CTC's \"*blank token*\". The \"blank token\" is a core component of the CTC algorithm. For more information, please take a look at the \"Alignment\" section [here](https://distill.pub/2017/ctc/)."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:17:22.12382Z","iopub.status.busy":"2022-07-02T01:17:22.123221Z","iopub.status.idle":"2022-07-02T01:17:22.134839Z","shell.execute_reply":"2022-07-02T01:17:22.133774Z","shell.execute_reply.started":"2022-07-02T01:17:22.123782Z"},"id":"znF0bNunsjbl","outputId":"314ca989-0587-485e-d7d2-3e6efe2158ee","trusted":true},"outputs":[{"data":{"text/plain":["81"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["vocab_dict[\"[UNK]\"] = len(vocab_dict)\n","vocab_dict[\"[PAD]\"] = len(vocab_dict)\n","len(vocab_dict)"]},{"cell_type":"markdown","metadata":{"id":"1CujRgBNVRaD"},"source":["Let's now save the vocabulary as a json file."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:17:22.136833Z","iopub.status.busy":"2022-07-02T01:17:22.136585Z","iopub.status.idle":"2022-07-02T01:17:22.144604Z","shell.execute_reply":"2022-07-02T01:17:22.143698Z","shell.execute_reply.started":"2022-07-02T01:17:22.136811Z"},"id":"ehyUoh9vk191","trusted":true},"outputs":[],"source":["import json\n","with open('vocab.json', 'w') as vocab_file:\n","    json.dump(vocab_dict, vocab_file)"]},{"cell_type":"markdown","metadata":{"id":"SHJDaKlIVVim"},"source":["In a final step, we use the json file to load the vocabulary into an instance of the `Wav2Vec2CTCTokenizer` class."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:17:22.146582Z","iopub.status.busy":"2022-07-02T01:17:22.146098Z","iopub.status.idle":"2022-07-02T01:17:22.563873Z","shell.execute_reply":"2022-07-02T01:17:22.562982Z","shell.execute_reply.started":"2022-07-02T01:17:22.146542Z"},"id":"xriFGEWQkO4M","outputId":"95e57a04-c48e-4748-8d77-bac71dd2750e","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["from transformers import Wav2Vec2CTCTokenizer\n","\n","tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"]},{"cell_type":"markdown","metadata":{"id":"mYcIiR2FQ96i"},"source":["### Create `Wav2Vec2FeatureExtractor`"]},{"cell_type":"markdown","metadata":{"id":"Y6mDEyW719rx"},"source":["Speech is a continuous signal and to be treated by computers, it first has to be discretized, which is usually called **sampling**. The sampling rate hereby plays an important role in that it defines how many data points of the speech signal are measured per second. Therefore, sampling with a higher sampling rate results in a better approximation of the *real* speech signal but also necessitates more values per second.\n","\n","A pretrained checkpoint expects its input data to have been sampled more or less from the same distribution as the data it was trained on. The same speech signals sampled at two different rates have a very different distribution, *e.g.*, doubling the sampling rate results in data points being twice as long. Thus, \n","before fine-tuning a pretrained checkpoint of an ASR model, it is crucial to verify that the sampling rate of the data that was used to pretrain the model matches the sampling rate of the dataset used to fine-tune the model.\n","\n","XLS-R was pretrained on audio data of [Babel](http://www.reading.ac.uk/AcaDepts/ll/speechlab/babel/r), \n","[Multilingual LibriSpeech (MLS)](https://huggingface.co/datasets/multilingual_librispeech), [Common Voice](https://huggingface.co/datasets/common_voice), [VoxPopuli](https://arxiv.org/abs/2101.00390), and [VoxLingua107](https://arxiv.org/abs/2011.12998) at a sampling rate of 16kHz. Common Voice, in its original form, has a sampling rate of 48kHz, thus we will have to downsample the fine-tuning data to 16kHz in the following.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KuUbPW7oV-B5"},"source":["A `Wav2Vec2FeatureExtractor` object requires the following parameters to be instantiated:\n","\n","- `feature_size`: Speech models take a sequence of feature vectors as an input. While the length of this sequence obviously varies, the feature size should not. In the case of Wav2Vec2, the feature size is 1 because the model was trained on the raw speech signal ${}^2$.\n","- `sampling_rate`: The sampling rate at which the model is trained on.\n","- `padding_value`: For batched inference, shorter inputs need to be padded with a specific value\n","- `do_normalize`: Whether the input should be *zero-mean-unit-variance* normalized or not. Usually, speech models perform better when normalizing the input\n","- `return_attention_mask`: Whether the model should make use of an `attention_mask` for batched inference. In general, XLS-R models checkpoints should **always** use the `attention_mask`."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:43:17.573382Z","iopub.status.busy":"2022-07-02T01:43:17.573013Z","iopub.status.idle":"2022-07-02T01:43:17.584668Z","shell.execute_reply":"2022-07-02T01:43:17.583718Z","shell.execute_reply.started":"2022-07-02T01:43:17.573353Z"},"id":"kAR0-2KLkopp","trusted":true},"outputs":[],"source":["from transformers import Wav2Vec2FeatureExtractor\n","\n","feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"]},{"cell_type":"markdown","metadata":{"id":"qUETetgqYC3W"},"source":["Great, XLS-R's feature extraction pipeline is thereby fully defined!\n","\n","For improved user-friendliness, the feature extractor and tokenizer are *wrapped* into a single `Wav2Vec2Processor` class so that one only needs a `model` and `processor` object."]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:43:20.315558Z","iopub.status.busy":"2022-07-02T01:43:20.315203Z","iopub.status.idle":"2022-07-02T01:43:20.326157Z","shell.execute_reply":"2022-07-02T01:43:20.325236Z","shell.execute_reply.started":"2022-07-02T01:43:20.315529Z"},"id":"KYZtoW-tlZgl","trusted":true},"outputs":[],"source":["from transformers import Wav2Vec2Processor\n","\n","processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"DrKnYuvDIoOO"},"source":["Next, we can prepare the dataset."]},{"cell_type":"markdown","metadata":{"id":"YFmShnl7RE35"},"source":["### Preprocess Data\n","\n","So far, we have not looked at the actual values of the speech signal but just the transcription. In addition to `sentence`, our datasets include two more column names `path` and `audio`. `path` states the absolute path of the audio file. Let's take a look.\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:43:25.739427Z","iopub.status.busy":"2022-07-02T01:43:25.739052Z","iopub.status.idle":"2022-07-02T01:43:26.130141Z","shell.execute_reply":"2022-07-02T01:43:26.129242Z","shell.execute_reply.started":"2022-07-02T01:43:25.739377Z"},"id":"TTCS7W6XJ9BG","outputId":"d4b375f8-a536-4be9-b995-578fd321acfe","trusted":true},"outputs":[{"ename":"ImportError","evalue":"To support decoding 'mp3' audio files, please install 'sox'.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\tonmo\\.conda\\envs\\open-mmlab\\lib\\site-packages\\datasets\\features\\audio.py:269\u001b[0m, in \u001b[0;36mAudio._decode_mp3\u001b[1;34m(self, path_or_file)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     torchaudio\u001b[39m.\u001b[39;49mset_audio_backend(\u001b[39m\"\u001b[39;49m\u001b[39msox_io\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    270\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n","File \u001b[1;32mc:\\Users\\tonmo\\.conda\\envs\\open-mmlab\\lib\\site-packages\\torchaudio\\backend\\utils.py:44\u001b[0m, in \u001b[0;36mset_audio_backend\u001b[1;34m(backend)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m backend \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m backend \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m list_audio_backends():\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBackend \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mbackend\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is not one of \u001b[39m\u001b[39m'\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mavailable backends: \u001b[39m\u001b[39m{\u001b[39;00mlist_audio_backends()\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[39mif\u001b[39;00m backend \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[1;31mRuntimeError\u001b[0m: Backend \"sox_io\" is not one of available backends: ['soundfile'].","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32me:\\kaggle\\DLSprint_challenge\\scripst\\wave2vec2-starter-for-dl-sprint-commonvoice.ipynb Cell 39'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/kaggle/DLSprint_challenge/scripst/wave2vec2-starter-for-dl-sprint-commonvoice.ipynb#ch0000038?line=0'>1</a>\u001b[0m common_voice_train[\u001b[39m0\u001b[39;49m][\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m]\n","File \u001b[1;32mc:\\Users\\tonmo\\.conda\\envs\\open-mmlab\\lib\\site-packages\\datasets\\arrow_dataset.py:2154\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2153\u001b[0m     \u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2154\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(\n\u001b[0;32m   2155\u001b[0m         key,\n\u001b[0;32m   2156\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\tonmo\\.conda\\envs\\open-mmlab\\lib\\site-packages\\datasets\\arrow_dataset.py:2139\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   2137\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, decoded\u001b[39m=\u001b[39mdecoded, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2138\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data, key, indices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 2139\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[0;32m   2140\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39;49mformatter, format_columns\u001b[39m=\u001b[39;49mformat_columns, output_all_columns\u001b[39m=\u001b[39;49moutput_all_columns\n\u001b[0;32m   2141\u001b[0m )\n\u001b[0;32m   2142\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n","File \u001b[1;32mc:\\Users\\tonmo\\.conda\\envs\\open-mmlab\\lib\\site-packages\\datasets\\formatting\\formatting.py:532\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    530\u001b[0m python_formatter \u001b[39m=\u001b[39m PythonFormatter(features\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    531\u001b[0m \u001b[39mif\u001b[39;00m format_columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 532\u001b[0m     \u001b[39mreturn\u001b[39;00m formatter(pa_table, query_type\u001b[39m=\u001b[39;49mquery_type)\n\u001b[0;32m    533\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    534\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m format_columns:\n","File \u001b[1;32mc:\\Users\\tonmo\\.conda\\envs\\open-mmlab\\lib\\site-packages\\datasets\\formatting\\formatting.py:281\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable, query_type: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 281\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_row(pa_table)\n\u001b[0;32m    282\u001b[0m     \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    283\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_column(pa_table)\n","File \u001b[1;32mc:\\Users\\tonmo\\.conda\\envs\\open-mmlab\\lib\\site-packages\\datasets\\formatting\\formatting.py:312\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    310\u001b[0m row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_arrow_extractor()\u001b[39m.\u001b[39mextract_row(pa_table)\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoded:\n\u001b[1;32m--> 312\u001b[0m     row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpython_features_decoder\u001b[39m.\u001b[39;49mdecode_row(row)\n\u001b[0;32m    313\u001b[0m \u001b[39mreturn\u001b[39;00m row\n","File \u001b[1;32mc:\\Users\\tonmo\\.conda\\envs\\open-mmlab\\lib\\site-packages\\datasets\\formatting\\formatting.py:221\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_row\u001b[1;34m(self, row)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode_row\u001b[39m(\u001b[39mself\u001b[39m, row: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[1;32m--> 221\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures\u001b[39m.\u001b[39;49mdecode_example(row) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39melse\u001b[39;00m row\n","File \u001b[1;32mc:\\Users\\tonmo\\.conda\\envs\\open-mmlab\\lib\\site-packages\\datasets\\features\\features.py:1612\u001b[0m, in \u001b[0;36mFeatures.decode_example\u001b[1;34m(self, example, token_per_repo_id)\u001b[0m\n\u001b[0;32m   1599\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode_example\u001b[39m(\u001b[39mself\u001b[39m, example: \u001b[39mdict\u001b[39m, token_per_repo_id\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1600\u001b[0m     \u001b[39m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m \n\u001b[0;32m   1602\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1609\u001b[0m \u001b[39m        :obj:`dict[str, Any]`\u001b[39;00m\n\u001b[0;32m   1610\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1612\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m   1613\u001b[0m         column_name: decode_nested_example(feature, value, token_per_repo_id\u001b[39m=\u001b[39mtoken_per_repo_id)\n\u001b[0;32m   1614\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[0;32m   1615\u001b[0m         \u001b[39melse\u001b[39;00m value\n\u001b[0;32m   1616\u001b[0m         \u001b[39mfor\u001b[39;00m column_name, (feature, value) \u001b[39min\u001b[39;00m zip_dict(\n\u001b[0;32m   1617\u001b[0m             {key: value \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m example}, example\n\u001b[0;32m   1618\u001b[0m         )\n\u001b[0;32m   1619\u001b[0m     }\n","File \u001b[1;32mc:\\Users\\tonmo\\.conda\\envs\\open-mmlab\\lib\\site-packages\\datasets\\features\\features.py:1613\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1599\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode_example\u001b[39m(\u001b[39mself\u001b[39m, example: \u001b[39mdict\u001b[39m, token_per_repo_id\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1600\u001b[0m     \u001b[39m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m \n\u001b[0;32m   1602\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1609\u001b[0m \u001b[39m        :obj:`dict[str, Any]`\u001b[39;00m\n\u001b[0;32m   1610\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m-> 1613\u001b[0m         column_name: decode_nested_example(feature, value, token_per_repo_id\u001b[39m=\u001b[39;49mtoken_per_repo_id)\n\u001b[0;32m   1614\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[0;32m   1615\u001b[0m         \u001b[39melse\u001b[39;00m value\n\u001b[0;32m   1616\u001b[0m         \u001b[39mfor\u001b[39;00m column_name, (feature, value) \u001b[39min\u001b[39;00m zip_dict(\n\u001b[0;32m   1617\u001b[0m             {key: value \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m example}, example\n\u001b[0;32m   1618\u001b[0m         )\n\u001b[0;32m   1619\u001b[0m     }\n","File \u001b[1;32mc:\\Users\\tonmo\\.conda\\envs\\open-mmlab\\lib\\site-packages\\datasets\\features\\features.py:1243\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[1;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[0;32m   1240\u001b[0m \u001b[39m# Object with special decoding:\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (Audio, Image)):\n\u001b[0;32m   1242\u001b[0m     \u001b[39m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[1;32m-> 1243\u001b[0m     \u001b[39mreturn\u001b[39;00m schema\u001b[39m.\u001b[39;49mdecode_example(obj, token_per_repo_id\u001b[39m=\u001b[39;49mtoken_per_repo_id) \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n","File \u001b[1;32mc:\\Users\\tonmo\\.conda\\envs\\open-mmlab\\lib\\site-packages\\datasets\\features\\audio.py:126\u001b[0m, in \u001b[0;36mAudio.decode_example\u001b[1;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn audio sample should have one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbytes\u001b[39m\u001b[39m'\u001b[39m\u001b[39m but both are None in \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[39melif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m path\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39mmp3\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 126\u001b[0m     array, sampling_rate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode_mp3(file \u001b[39mif\u001b[39;49;00m file \u001b[39melse\u001b[39;49;00m path)\n\u001b[0;32m    127\u001b[0m \u001b[39melif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m path\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39mopus\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m file:\n","File \u001b[1;32mc:\\Users\\tonmo\\.conda\\envs\\open-mmlab\\lib\\site-packages\\datasets\\features\\audio.py:271\u001b[0m, in \u001b[0;36mAudio._decode_mp3\u001b[1;34m(self, path_or_file)\u001b[0m\n\u001b[0;32m    269\u001b[0m     torchaudio\u001b[39m.\u001b[39mset_audio_backend(\u001b[39m\"\u001b[39m\u001b[39msox_io\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    270\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 271\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTo support decoding \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmp3\u001b[39m\u001b[39m'\u001b[39m\u001b[39m audio files, please install \u001b[39m\u001b[39m'\u001b[39m\u001b[39msox\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    273\u001b[0m array, sampling_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mload(path_or_file, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmp3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_rate \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_rate \u001b[39m!=\u001b[39m sampling_rate:\n","\u001b[1;31mImportError\u001b[0m: To support decoding 'mp3' audio files, please install 'sox'."]}],"source":["common_voice_train[0][\"path\"]"]},{"cell_type":"markdown","metadata":{"id":"T6ndIjHGFp0W"},"source":["XLS-R expects the input in the format of a 1-dimensional array of 16 kHz. This means that the audio file has to be loaded and resampled.\n","\n"," Thankfully, `datasets` does this automatically by calling the other column `audio`. Let try it out. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:43:28.949297Z","iopub.status.busy":"2022-07-02T01:43:28.948265Z","iopub.status.idle":"2022-07-02T01:43:28.97438Z","shell.execute_reply":"2022-07-02T01:43:28.973387Z","shell.execute_reply.started":"2022-07-02T01:43:28.949246Z"},"id":"qj_z5Zc3GAs9","outputId":"6b0dca9c-aadc-493f-f754-b01d0f35fe28","trusted":true},"outputs":[],"source":["common_voice_train[0][\"audio\"]"]},{"cell_type":"markdown","metadata":{"id":"WUUTgI1bGHW-"},"source":["Great, we can see that the audio file has automatically been loaded. This is thanks to the new [`\"Audio\"` feature](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=audio#datasets.Audio) introduced in `datasets == 4.13.3`, which loads and resamples audio files on-the-fly upon calling.\n","\n","In the example above we can see that the audio data is loaded with a sampling rate of 48kHz whereas 16kHz are expected by the model. We can set the audio feature to the correct sampling rate by making use of [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column):"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:43:36.699011Z","iopub.status.busy":"2022-07-02T01:43:36.698313Z","iopub.status.idle":"2022-07-02T01:43:36.713175Z","shell.execute_reply":"2022-07-02T01:43:36.712062Z","shell.execute_reply.started":"2022-07-02T01:43:36.698975Z"},"id":"rrv65aj7G95i","trusted":true},"outputs":[],"source":["common_voice_train = common_voice_train.cast_column(\"audio\", Audio(sampling_rate=16_000))\n","common_voice_test = common_voice_test.cast_column(\"audio\", Audio(sampling_rate=16_000))"]},{"cell_type":"markdown","metadata":{"id":"PcnO4x-NGBEi"},"source":["Let's take a look at `\"audio\"` again."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:43:40.01332Z","iopub.status.busy":"2022-07-02T01:43:40.012956Z","iopub.status.idle":"2022-07-02T01:43:40.048486Z","shell.execute_reply":"2022-07-02T01:43:40.047463Z","shell.execute_reply.started":"2022-07-02T01:43:40.013289Z"},"id":"aKtkc1o_HWHC","outputId":"c2c1375e-4812-4112-d843-9e3da6dd327d","trusted":true},"outputs":[],"source":["common_voice_train[0][\"audio\"]"]},{"cell_type":"markdown","metadata":{"id":"SOckzFd4Mbzq"},"source":["This seemed to have worked! Let's listen to a couple of audio files to better understand the dataset and verify that the audio was correctly loaded. \n","\n","**Note**: *You can click the following cell a couple of times to listen to different speech samples.*"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:43:44.283163Z","iopub.status.busy":"2022-07-02T01:43:44.282817Z","iopub.status.idle":"2022-07-02T01:43:44.402438Z","shell.execute_reply":"2022-07-02T01:43:44.401487Z","shell.execute_reply.started":"2022-07-02T01:43:44.283134Z"},"id":"dueM6U7Ev0OA","outputId":"1a771d0a-085d-441b-cc1b-f05438425ce4","trusted":true},"outputs":[],"source":["import IPython.display as ipd\n","import numpy as np\n","import random\n","\n","rand_int = random.randint(0, len(common_voice_train)-1)\n","\n","print(common_voice_train[rand_int][\"sentence\"])\n","ipd.Audio(data=common_voice_train[rand_int][\"audio\"][\"array\"], autoplay=True, rate=16000)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:43:52.018142Z","iopub.status.busy":"2022-07-02T01:43:52.017783Z","iopub.status.idle":"2022-07-02T01:43:52.079481Z","shell.execute_reply":"2022-07-02T01:43:52.078458Z","shell.execute_reply.started":"2022-07-02T01:43:52.018111Z"},"id":"1Po2g7YPuRTx","outputId":"63478ae9-2927-4ec1-c13c-41fb3754e18e","trusted":true},"outputs":[],"source":["rand_int = random.randint(0, len(common_voice_train)-1)\n","\n","print(\"Target text:\", common_voice_train[rand_int][\"sentence\"])\n","print(\"Input array shape:\", common_voice_train[rand_int][\"audio\"][\"array\"].shape)\n","print(\"Sampling rate:\", common_voice_train[rand_int][\"audio\"][\"sampling_rate\"])"]},{"cell_type":"markdown","metadata":{"id":"k3Pbn5WvOYZF"},"source":["Finally, we can leverage `Wav2Vec2Processor` to process the data to the format expected by `Wav2Vec2ForCTC` for training. To do so let's make use of Dataset's [`map(...)`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=map#datasets.DatasetDict.map) function.\n","\n","First, we load and resample the audio data, simply by calling `batch[\"audio\"]`.\n","Second, we extract the `input_values` from the loaded audio file. In our case, the `Wav2Vec2Processor` only normalizes the data. For other speech models, however, this step can include more complex feature extraction, such as [Log-Mel feature extraction](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum). \n","Third, we encode the transcriptions to label ids.\n","\n","**Note**: This mapping function is a good example of how the `Wav2Vec2Processor` class should be used. In \"normal\" context, calling `processor(...)` is redirected to `Wav2Vec2FeatureExtractor`'s call method. When wrapping the processor into the `as_target_processor` context, however, the same method is redirected to `Wav2Vec2CTCTokenizer`'s call method.\n","For more information please check the [docs](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#transformers.Wav2Vec2Processor.__call__)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:44:02.957513Z","iopub.status.busy":"2022-07-02T01:44:02.95715Z","iopub.status.idle":"2022-07-02T01:44:02.963222Z","shell.execute_reply":"2022-07-02T01:44:02.962289Z","shell.execute_reply.started":"2022-07-02T01:44:02.957483Z"},"id":"eJY7I0XAwe9p","trusted":true},"outputs":[],"source":["def prepare_dataset(batch):\n","    audio = batch[\"audio\"]\n","\n","    # batched output is \"un-batched\"\n","    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n","    batch[\"input_length\"] = len(batch[\"input_values\"])\n","    \n","    with processor.as_target_processor():\n","        batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n","    return batch"]},{"cell_type":"markdown","metadata":{"id":"q6Pg_WR3OGAP"},"source":["Let's apply the data preparation function to all examples."]},{"cell_type":"markdown","metadata":{},"source":["### Sample Training Example\n","train samples=10000\n","val samples=1000"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:44:08.414582Z","iopub.status.busy":"2022-07-02T01:44:08.414227Z","iopub.status.idle":"2022-07-02T01:44:08.432388Z","shell.execute_reply":"2022-07-02T01:44:08.431442Z","shell.execute_reply.started":"2022-07-02T01:44:08.414552Z"},"trusted":true},"outputs":[],"source":["common_voice_train=common_voice_train.select(range(10000))\n","common_voice_test=common_voice_test.select(range(1000))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:44:11.459946Z","iopub.status.busy":"2022-07-02T01:44:11.459279Z","iopub.status.idle":"2022-07-02T01:46:31.98268Z","shell.execute_reply":"2022-07-02T01:46:31.981349Z","shell.execute_reply.started":"2022-07-02T01:44:11.45991Z"},"id":"-np9xYK-wl8q","outputId":"00d6940a-a7bf-4128-896b-76bc289e5b7f","trusted":true},"outputs":[],"source":["common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names)\n","common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names)"]},{"cell_type":"markdown","metadata":{"id":"nKcEWHvKI1by"},"source":["**Note**: Currently `datasets` make use of [`torchaudio`](https://pytorch.org/audio/stable/index.html) and [`librosa`](https://librosa.org/doc/latest/index.html) for audio loading and resampling. If you wish to implement your own costumized data loading/sampling, feel free to just make use of the `\"path\"` column instead and disregard the `\"audio\"` column."]},{"cell_type":"markdown","metadata":{"id":"24CxHd5ewI4T"},"source":["Long input sequences require a lot of memory. XLS-R is based on `self-attention` the memory requirement scales quadratically with the input length for long input sequences (*cf.* with [this](https://www.reddit.com/r/MachineLearning/comments/genjvb/d_why_is_the_maximum_input_sequence_length_of/) reddit post). In case this demo crashes with an \"Out-of-memory\" error for you, you might want to uncomment the following lines to filter all sequences that are longer than 5 seconds for training."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-01T23:43:19.240524Z","iopub.status.busy":"2022-07-01T23:43:19.237266Z","iopub.status.idle":"2022-07-01T23:43:19.247887Z","shell.execute_reply":"2022-07-01T23:43:19.246332Z","shell.execute_reply.started":"2022-07-01T23:43:19.240478Z"},"id":"tdHfbUJ_09iA","trusted":true},"outputs":[],"source":["#max_input_length_in_sec = 5.0\n","#common_voice_train = common_voice_train.filter(lambda x: x < max_input_length_in_sec * processor.feature_extractor.sampling_rate, input_columns=[\"input_length\"])"]},{"cell_type":"markdown","metadata":{"id":"1ZWDCCKqwcfS"},"source":["Awesome, now we are ready to start training!"]},{"cell_type":"markdown","metadata":{"id":"gYlQkKVoRUos"},"source":["## Training\n","\n","The data is processed so that we are ready to start setting up the training pipeline. We will make use of 🤗's [Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) for which we essentially need to do the following:\n","\n","- Define a data collator. In contrast to most NLP models, XLS-R has a much larger input length than output length. *E.g.*, a sample of input length 50000 has an output length of no more than 100. Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only be padded to the longest sample in their batch and not the overall longest sample. Therefore, fine-tuning XLS-R requires a special padding data collator, which we will define below\n","\n","- Evaluation metric. During training, the model should be evaluated on the word error rate. We should define a `compute_metrics` function accordingly\n","\n","- Load a pretrained checkpoint. We need to load a pretrained checkpoint and configure it correctly for training.\n","\n","- Define the training configuration.\n","\n","After having fine-tuned the model, we will correctly evaluate it on the test data and verify that it has indeed learned to correctly transcribe speech."]},{"cell_type":"markdown","metadata":{"id":"Slk403unUS91"},"source":["### Set-up Trainer\n","\n","Let's start by defining the data collator. The code for the data collator was copied from [this example](https://github.com/huggingface/transformers/blob/7e61d56a45c19284cfda0cee8995fb552f6b1f4e/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py#L219).\n","\n","Without going into too many details, in contrast to the common data collators, this data collator treats the `input_values` and `labels` differently and thus applies to separate padding functions on them (again making use of XLS-R processor's context manager). This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function.\n","Analogous to the common data collators, the padding tokens in the labels with `-100` so that those tokens are **not** taken into account when computing the loss."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:46:43.447904Z","iopub.status.busy":"2022-07-02T01:46:43.4471Z","iopub.status.idle":"2022-07-02T01:46:43.462404Z","shell.execute_reply":"2022-07-02T01:46:43.461446Z","shell.execute_reply.started":"2022-07-02T01:46:43.447867Z"},"id":"tborvC9hx88e","trusted":true},"outputs":[],"source":["import torch\n","\n","from dataclasses import dataclass, field\n","from typing import Any, Dict, List, Optional, Union\n","\n","@dataclass\n","class DataCollatorCTCWithPadding:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs received.\n","    Args:\n","        processor (:class:`~transformers.Wav2Vec2Processor`)\n","            The processor used for proccessing the data.\n","        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n","              sequence if provided).\n","            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n","              maximum acceptable input length for the model if that argument is not provided.\n","            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n","              different lengths).\n","    \"\"\"\n","\n","    processor: Wav2Vec2Processor\n","    padding: Union[bool, str] = True\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lenghts and need\n","        # different padding methods\n","        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","\n","        batch = self.processor.pad(\n","            input_features,\n","            padding=self.padding,\n","            return_tensors=\"pt\",\n","        )\n","        with self.processor.as_target_processor():\n","            labels_batch = self.processor.pad(\n","                label_features,\n","                padding=self.padding,\n","                return_tensors=\"pt\",\n","            )\n","\n","        # replace padding with -100 to ignore loss correctly\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:46:46.131111Z","iopub.status.busy":"2022-07-02T01:46:46.130482Z","iopub.status.idle":"2022-07-02T01:46:46.13631Z","shell.execute_reply":"2022-07-02T01:46:46.135102Z","shell.execute_reply.started":"2022-07-02T01:46:46.131058Z"},"id":"lbQf5GuZyQ4_","trusted":true},"outputs":[],"source":["data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"]},{"cell_type":"markdown","metadata":{"id":"xO-Zdj-5cxXp"},"source":["Next, the evaluation metric is defined. As mentioned earlier, the \n","predominant metric in ASR is the word error rate (WER), hence we will use it in this notebook as well."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:46:49.210814Z","iopub.status.busy":"2022-07-02T01:46:49.209912Z","iopub.status.idle":"2022-07-02T01:46:50.114728Z","shell.execute_reply":"2022-07-02T01:46:50.113749Z","shell.execute_reply.started":"2022-07-02T01:46:49.210764Z"},"id":"9Xsux2gmyXso","outputId":"18ceeb9e-1a0d-4ee8-f511-a12ad3608bf1","trusted":true},"outputs":[],"source":["wer_metric = load_metric(\"wer\")"]},{"cell_type":"markdown","metadata":{"id":"E1qZU5p-deqB"},"source":["The model will return a sequence of logit vectors:\n","$\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ with $\\mathbf{y}_1 = f_{\\theta}(x_1, \\ldots, x_n)[0]$ and $n >> m$.\n","\n","A logit vector $\\mathbf{y}_1$ contains the log-odds for each word in the vocabulary we defined earlier, thus $\\text{len}(\\mathbf{y}_i) =$ `config.vocab_size`. We are interested in the most likely prediction of the model and thus take the `argmax(...)` of the logits. Also, we transform the encoded labels back to the original string by replacing `-100` with the `pad_token_id` and decoding the ids while making sure that consecutive tokens are **not** grouped to the same token in CTC style ${}^1$."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:46:57.78489Z","iopub.status.busy":"2022-07-02T01:46:57.783975Z","iopub.status.idle":"2022-07-02T01:46:57.792698Z","shell.execute_reply":"2022-07-02T01:46:57.790854Z","shell.execute_reply.started":"2022-07-02T01:46:57.784836Z"},"id":"1XZ-kjweyTy_","trusted":true},"outputs":[],"source":["def compute_metrics(pred):\n","    pred_logits = pred.predictions\n","    pred_ids = np.argmax(pred_logits, axis=-1)\n","\n","    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n","\n","    pred_str = processor.batch_decode(pred_ids)\n","    # we do not want to group tokens when computing the metrics\n","    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n","\n","    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer}"]},{"cell_type":"markdown","metadata":{"id":"Xmgrx4bRwLIH"},"source":["Now, we can load the pretrained checkpoint of [Wav2Vec2-XLS-R-300M](https://huggingface.co/facebook/wav2vec2-xls-r-300m). The tokenizer's `pad_token_id` must be to define the model's `pad_token_id` or in the case of `Wav2Vec2ForCTC` also CTC's *blank token* ${}^2$. To save GPU memory, we enable PyTorch's [gradient checkpointing](https://pytorch.org/docs/stable/checkpoint.html) and also set the loss reduction to \"*mean*\".\n","\n","**Note**: When using this notebook the hyper-parameter settings might not work very well. Feel free to adapt those depending on your use case. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T01:47:09.483888Z","iopub.status.busy":"2022-07-02T01:47:09.483264Z","iopub.status.idle":"2022-07-02T01:48:12.269051Z","shell.execute_reply":"2022-07-02T01:48:12.267939Z","shell.execute_reply.started":"2022-07-02T01:47:09.483849Z"},"id":"e7cqAWIayn6w","outputId":"3e6cebea-78ef-45df-87b0-f63b78ba9644","trusted":true},"outputs":[],"source":["from transformers import Wav2Vec2ForCTC\n","\n","model = Wav2Vec2ForCTC.from_pretrained(\n","    \"facebook/wav2vec2-xls-r-300m\", \n","    attention_dropout=0.0,\n","    hidden_dropout=0.0,\n","    feat_proj_dropout=0.0,\n","    mask_time_prob=0.05,\n","    layerdrop=0.0,\n","    ctc_loss_reduction=\"mean\", \n","    pad_token_id=processor.tokenizer.pad_token_id,\n","    vocab_size=len(processor.tokenizer),\n",")"]},{"cell_type":"markdown","metadata":{"id":"1DwR3XLSzGDD"},"source":["The first component of XLS-R consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. This part of the model has already been sufficiently trained during pretraining and as stated in the [paper](https://arxiv.org/pdf/2006.13979.pdf) does not need to be fine-tuned anymore. \n","Thus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T02:01:47.754647Z","iopub.status.busy":"2022-07-02T02:01:47.753811Z","iopub.status.idle":"2022-07-02T02:01:47.762862Z","shell.execute_reply":"2022-07-02T02:01:47.761473Z","shell.execute_reply.started":"2022-07-02T02:01:47.754608Z"},"id":"oGI8zObtZ3V0","trusted":true},"outputs":[],"source":["model.freeze_feature_extractor()"]},{"cell_type":"markdown","metadata":{"id":"lD4aGhQM0K-D"},"source":["In a final step, we define all parameters related to training. \n","To give more explanation on some of the parameters:\n","- `group_by_length` makes training more efficient by grouping training samples of similar input length into one batch. This can significantly speed up training time by heavily reducing the overall number of useless padding tokens that are passed through the model\n","- `learning_rate` and `weight_decay` were heuristically tuned until fine-tuning has become stable. Note that those parameters strongly depend on the Common Voice dataset and might be suboptimal for other speech datasets.\n","\n","For more explanations on other parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T02:01:53.128777Z","iopub.status.busy":"2022-07-02T02:01:53.127593Z","iopub.status.idle":"2022-07-02T02:01:53.27295Z","shell.execute_reply":"2022-07-02T02:01:53.271962Z","shell.execute_reply.started":"2022-07-02T02:01:53.128727Z"},"id":"KbeKSV7uzGPP","trusted":true},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","step_n = 20 # Change it to 400 while training.\n","\n","training_args = TrainingArguments(\n","  output_dir=\"./\",\n","  group_by_length=True,\n","  per_device_train_batch_size=16,\n","  gradient_accumulation_steps=2,\n","  evaluation_strategy=\"steps\",\n","  num_train_epochs=30,\n","  gradient_checkpointing=True,\n","  fp16=True,\n","  save_steps=step_n,\n","  eval_steps=step_n,\n","  logging_steps=step_n,\n","  learning_rate=3e-4,\n","  warmup_steps=500,\n","  save_total_limit=2,\n","  push_to_hub=False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"OsW-WZcL1ZtN"},"source":["Now, all instances can be passed to Trainer and we are ready to start training!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T02:02:00.447097Z","iopub.status.busy":"2022-07-02T02:02:00.446722Z","iopub.status.idle":"2022-07-02T02:02:07.74064Z","shell.execute_reply":"2022-07-02T02:02:07.739576Z","shell.execute_reply.started":"2022-07-02T02:02:00.447066Z"},"id":"rY7vBmFCPFgC","outputId":"c47ecc78-5259-44db-c121-5bd7945defb8","trusted":true},"outputs":[],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=common_voice_train,\n","    eval_dataset=common_voice_test,\n","    tokenizer=processor.feature_extractor,\n",")"]},{"cell_type":"markdown","metadata":{"id":"rpvZHM1xReIW"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T02:02:07.7451Z","iopub.status.busy":"2022-07-02T02:02:07.744227Z","iopub.status.idle":"2022-07-02T02:02:07.751777Z","shell.execute_reply":"2022-07-02T02:02:07.74985Z","shell.execute_reply.started":"2022-07-02T02:02:07.74507Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-02T02:02:16.787366Z","iopub.status.busy":"2022-07-02T02:02:16.786975Z","iopub.status.idle":"2022-07-02T02:24:47.539267Z","shell.execute_reply":"2022-07-02T02:24:47.536887Z","shell.execute_reply.started":"2022-07-02T02:02:16.787334Z"},"id":"9fRr9TG5pGBl","outputId":"122ea040-7b24-452a-c7d4-7e72e1c46973","trusted":true},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["# Resources:\n","* blog: https://huggingface.co/blog/fine-tune-xlsr-wav2vec2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!ls"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('open-mmlab')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"e1611e328d9ce540a72720ddfd7d51bd2ff21548f455827874db30256d5360e0"}}},"nbformat":4,"nbformat_minor":4}
